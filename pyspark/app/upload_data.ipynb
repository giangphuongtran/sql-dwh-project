{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "924b8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f52516df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlalchemy_engine = create_engine(\"postgresql://dbadmin:dbadminpw@localhost:5433/sqldatabase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ca5192d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x124dea050>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlalchemy_engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "71a7da10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schemas 'bronze' and 'silver' created (or already existed).\n"
     ]
    }
   ],
   "source": [
    "with sqlalchemy_engine.connect() as conn:\n",
    "    conn.execute(sqlalchemy.text(\"CREATE SCHEMA IF NOT EXISTS bronze\"))\n",
    "    conn.execute(sqlalchemy.text(\"CREATE SCHEMA IF NOT EXISTS silver\"))\n",
    "    conn.commit() # Commit the changes if you're not in an auto-commit context\n",
    "    print(\"Schemas 'bronze' and 'silver' created (or already existed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6828358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://localhost:5433/sqldatabase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "31e4e7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/23 21:10:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ETL CSV to Postgres\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4f9a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_table_map = {\n",
    "        \"cust_info.csv\": \"crm_cust_info\",\n",
    "        \"prd_info.csv\": \"crm_prd_info\",\n",
    "        \"sales_details.csv\": \"crm_sales_details\",\n",
    "        \"CUST_AZ12.csv\": \"erp_cust_info\",\n",
    "        \"LOC_A101.csv\": \"erp_loc_info\",\n",
    "        \"PX_CAT_G1V2.csv\": \"erp_px_cat\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8e83d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"source_crm\", \"source_erp\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54adc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/source_crm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m folders:\n\u001b[32m      5\u001b[39m         full_path = os.path.join(\u001b[33m\"\u001b[39m\u001b[33m/data\u001b[39m\u001b[33m\"\u001b[39m, folder)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      7\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m file_table_map:\n\u001b[32m      8\u001b[39m                 table = file_table_map[file]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/data/source_crm'"
     ]
    }
   ],
   "source": [
    "user = 'dbadmin'\n",
    "password = 'dbadminpw'\n",
    "file_path = '/Users/mac/Learningnewthings/data-engineering/projectFirst/datasets'\n",
    "\n",
    "for folder in folders:\n",
    "        full_path = os.path.join(file_path, folder)\n",
    "        for file in os.listdir(full_path):\n",
    "            if file.endswith('.csv') and file in file_table_map:\n",
    "                table = file_table_map[file]\n",
    "                file_path = os.path.join(full_path, file)\n",
    "                print(f\" â†’ File path: {file_path}\")\n",
    "                print(f\"Ingesting {file} to bronze.{table}\")\n",
    "                \n",
    "                df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "                \n",
    "                df.write \\\n",
    "                    .format('jdbc') \\\n",
    "                    .option('url', jdbc_url) \\\n",
    "                    .option('dbtable', f'bronze.{table}') \\\n",
    "                    .option('user', user) \\\n",
    "                    .option('password', password) \\\n",
    "                    .option('driver', 'org.postgresql.Driver') \\\n",
    "                    .mode('overwrite') \\\n",
    "                    .save()\n",
    "                    \n",
    "    # print('All CSVs ingested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2437450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".de-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
